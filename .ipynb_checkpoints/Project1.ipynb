{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02033e58-c459-4759-8565-0568d8a6c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (24.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (3.7.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6cd35de-9ca7-4b74-815f-62fe50c2cfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://1e11464daad1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYC Taxi Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa001a83550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, lag, lead, when, lit, expr,\n",
    "    sum as _sum, avg as _avg\n",
    ")\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Shapely imports\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "# 1) Start Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c057646-a160-420e-b9b8-a210cb90a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      "\n",
      "+--------------------------------+--------------------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|medallion                       |hack_license                    |vendor_id|rate_code|store_and_fwd_flag|pickup_datetime    |dropoff_datetime   |passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------------------+--------------------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|89D227B655E5C82AECF13C3F540D4CF4|BA96DE419E711691B9445D6A6307C170|CMT      |1        |N                 |2013-01-01 15:11:48|2013-01-01 15:18:10|4              |382              |1.0          |-73.978165      |40.757977      |-73.989838       |40.751171       |\n",
      "|0BD7C8F5BA12B88E0B67BED28BEA73D8|9FD8F69F0804BDB5549F40E9DA1BE472|CMT      |1        |N                 |2013-01-06 00:18:35|2013-01-06 00:22:54|1              |259              |1.5          |-74.006683      |40.731781      |-73.994499       |40.75066        |\n",
      "|0BD7C8F5BA12B88E0B67BED28BEA73D8|9FD8F69F0804BDB5549F40E9DA1BE472|CMT      |1        |N                 |2013-01-05 18:49:41|2013-01-05 18:54:23|1              |282              |1.1          |-74.004707      |40.73777       |-74.009834       |40.726002       |\n",
      "|DFD2202EE08F7A8DC9A57B02ACB81FE2|51EE87E3205C985EF8431D850C786310|CMT      |1        |N                 |2013-01-07 23:54:15|2013-01-07 23:58:20|2              |244              |0.7          |-73.974602      |40.759945      |-73.984734       |40.759388       |\n",
      "|DFD2202EE08F7A8DC9A57B02ACB81FE2|51EE87E3205C985EF8431D850C786310|CMT      |1        |N                 |2013-01-07 23:25:03|2013-01-07 23:34:24|1              |560              |2.1          |-73.97625       |40.748528      |-74.002586       |40.747868       |\n",
      "+--------------------------------+--------------------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hardcode the file path to your sample taxi CSV\n",
    "# TAXI_CSV_PATH = 'sample/Sample NYC Data.csv'\n",
    "\n",
    "# Load the CSV into a Spark DataFrame\n",
    "df_taxi_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('trip_data/trip_data_1.csv')\n",
    "\n",
    "# Show the schema and maybe a few rows\n",
    "df_taxi_raw.printSchema()\n",
    "df_taxi_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "462ee2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_raw = df_taxi_raw.drop(\"hack_license\" ,\"vendor_id\", \"rate_code\", \"passenger_count\", \"store_and_fwd_flag\",'trip_time_in_secs','trip_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa07875-ba7f-40f1-98c3-b53523fbfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_raw.write \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"trip_data/nyc_taxi_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb5e853-bcab-4561-916f-6d836583605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|    pickup_datetime|   dropoff_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+\n",
      "|6C8C5507F1928059F...|2013-01-07 05:59:28|2013-01-07 06:01:41|       -73.99041|       40.75613|       -73.983711|       40.765839|\n",
      "|A3281E8510FED7EE0...|2013-01-05 13:22:48|2013-01-05 13:25:35|      -73.977295|      40.750359|       -73.976913|       40.760567|\n",
      "|927C59F57F43537DA...|2013-01-05 10:41:08|2013-01-05 11:06:53|      -73.785355|      40.648205|       -73.941956|       40.714146|\n",
      "|FEBFB5478D15AE3E0...|2013-01-07 06:13:55|2013-01-07 06:19:30|      -73.965736|      40.768623|        -73.98288|       40.755615|\n",
      "|275AF4D0E47451563...|2013-01-06 01:45:51|2013-01-06 01:52:17|       -74.00087|      40.730759|       -73.983757|       40.759125|\n",
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi = spark.read.parquet(\"trip_data/nyc_taxi_parquet\")\n",
    "df_taxi.printSchema()\n",
    "df_taxi.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9743166f-8cc1-42f0-bd0d-0d79995e1c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded & broadcasted 104 borough polygons.\n"
     ]
    }
   ],
   "source": [
    "# Hardcode the file path to your borough GeoJSON\n",
    "GEOJSON_PATH = \"sample/nyc-boroughs.geojson\"\n",
    "\n",
    "# Read the borough boundaries from GeoJSON\n",
    "with open(GEOJSON_PATH, \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "features = geojson_data[\"features\"]\n",
    "\n",
    "# Function to get polygon area and borough code for sorting\n",
    "def polygon_area(feature):\n",
    "    return shape(feature[\"geometry\"]).area\n",
    "\n",
    "def borough_code(feature):\n",
    "    return feature[\"properties\"][\"boroughCode\"]\n",
    "\n",
    "# Sort features by (boroughCode ascending, area descending)\n",
    "features_sorted = sorted(\n",
    "    features,\n",
    "    key=lambda f: (borough_code(f), -polygon_area(f))\n",
    ")\n",
    "\n",
    "# Build list of (boroughName, shapelyShape)\n",
    "borough_polygons = []\n",
    "for f in features_sorted:\n",
    "    bname = f[\"properties\"][\"borough\"]\n",
    "    geom  = shape(f[\"geometry\"])\n",
    "    borough_polygons.append((bname, geom))\n",
    "\n",
    "# Broadcast to all executors\n",
    "borough_polygons_bc = spark.sparkContext.broadcast(borough_polygons)\n",
    "\n",
    "print(\"Loaded & broadcasted\", len(borough_polygons), \"borough polygons.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29b4088-f34c-4fb5-8e10-e547ba6f8a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+\n",
      "|medallion                       |pickup_datetime    |dropoff_datetime   |pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|pickup_borough|dropoff_borough|\n",
      "+--------------------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+\n",
      "|89D227B655E5C82AECF13C3F540D4CF4|2013-01-01 15:11:48|2013-01-01 15:18:10|-73.978165      |40.757977      |-73.989838       |40.751171       |Manhattan     |Manhattan      |\n",
      "|0BD7C8F5BA12B88E0B67BED28BEA73D8|2013-01-06 00:18:35|2013-01-06 00:22:54|-74.006683      |40.731781      |-73.994499       |40.75066        |Manhattan     |Manhattan      |\n",
      "|0BD7C8F5BA12B88E0B67BED28BEA73D8|2013-01-05 18:49:41|2013-01-05 18:54:23|-74.004707      |40.73777       |-74.009834       |40.726002       |Manhattan     |Manhattan      |\n",
      "+--------------------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,avg\n",
    "\n",
    "# Create a Python function to identify the borough using Shapely\n",
    "def find_borough(lon, lat):\n",
    "    if lon is None or lat is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    point = Point(lon, lat)\n",
    "    for (b_name, b_polygon) in borough_polygons_bc.value:\n",
    "        if b_polygon.contains(point):\n",
    "            return b_name\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Register this as a Spark UDF\n",
    "find_borough_udf = udf(find_borough, StringType())\n",
    "\n",
    "# Add pickup_borough and dropoff_borough columns\n",
    "df_taxi = df_taxi_raw \\\n",
    "    .withColumn(\"pickup_borough\", find_borough_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\"))) \\\n",
    "    .withColumn(\"dropoff_borough\", find_borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\\\n",
    "    # .withColumn(\"pickup_ts\", to_timestamp(col(\"pickup_datetime\"), \"DD-MM-yy HH:mm\"))\\\n",
    "    # .withColumn(\"dropoff_ts\", to_timestamp(col(\"dropoff_datetime\"), \"DD-MM-yy HH:mm\"))\n",
    "\n",
    "df_taxi.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8673858f-914c-46da-844a-a9396308b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+\n",
      "|pickup_datetime    |dropoff_datetime   |duration_sec|\n",
      "+-------------------+-------------------+------------+\n",
      "|2013-01-01 15:11:48|2013-01-01 15:18:10|382         |\n",
      "|2013-01-06 00:18:35|2013-01-06 00:22:54|259         |\n",
      "|2013-01-05 18:49:41|2013-01-05 18:54:23|282         |\n",
      "|2013-01-07 23:54:15|2013-01-07 23:58:20|245         |\n",
      "|2013-01-07 23:25:03|2013-01-07 23:34:24|561         |\n",
      "|2013-01-07 15:27:48|2013-01-07 15:38:37|649         |\n",
      "|2013-01-08 11:01:15|2013-01-08 11:08:14|419         |\n",
      "|2013-01-07 12:39:18|2013-01-07 13:10:56|1898        |\n",
      "|2013-01-07 18:15:47|2013-01-07 18:20:47|300         |\n",
      "|2013-01-07 15:33:28|2013-01-07 15:49:26|958         |\n",
      "+-------------------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Convert to proper TimestampType using the correct format\n",
    "\n",
    "# 3) Compute duration in seconds\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"duration_sec\",\n",
    "    unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))\n",
    ")\n",
    "# df_taxi.show(5)\n",
    "# 4) Filter out outliers\n",
    "df_taxi = df_taxi.filter((col(\"duration_sec\") >= 0) & (col(\"duration_sec\") <= 14400))\n",
    "\n",
    "df_taxi.select(\"pickup_datetime\", \"dropoff_datetime\", \"duration_sec\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd3324c-2377-4545-a0cf-bc8c696971cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- pickup_borough: string (nullable = true)\n",
      " |-- dropoff_borough: string (nullable = true)\n",
      " |-- duration_sec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c697795-33ec-43e4-9a2d-b2efc014c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query 1: Utilization per Taxi/Driver ===\n",
      "+--------------------------------+-------------------+---------------+-------------------+\n",
      "|medallion                       |total_occupied_time|total_idle_time|utilization        |\n",
      "+--------------------------------+-------------------+---------------+-------------------+\n",
      "|0038EF45118925A510975FD0CCD67192|780480             |1509360        |0.34084477518079864|\n",
      "|00BD5D1AD3A96C997E49E0453A6C5DF1|810120             |1080300        |0.42853968959278893|\n",
      "|01A2F4366180AEB433600BAEA196BFC7|990364             |1212008        |0.44968061708012996|\n",
      "|01D13A056D9A26F84C328DFDD5534B55|629460             |826080         |0.4324580568036605 |\n",
      "|01F24976B8E3FF46A08187C86F1F9AB7|375011             |227511         |0.6224021695473361 |\n",
      "|02063AF23344CEA458E992EC448C5E73|638880             |929160         |0.4074385857503635 |\n",
      "|024E99A049B748C443A541B2F6F55E5F|343440             |429240         |0.4444789563596832 |\n",
      "|025B4E80E8A06FDB0FC0A05E319B0E60|829481             |1133337        |0.422597000842666  |\n",
      "|026B27179DE85CFDC57E5D97372C63F7|406956             |474964         |0.46144321480406386|\n",
      "|02B196981B24858BCD38C205AA81D7D8|552780             |962040         |0.3649146433239593 |\n",
      "+--------------------------------+-------------------+---------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window spec: partition by medallion, order by pickup time\n",
    "w = Window.partitionBy(\"medallion\").orderBy(\"pickup_datetime\")\n",
    "\n",
    "# Get the dropoff time of the previous trip\n",
    "df_taxi = df_taxi.withColumn(\"prev_dropoff_datetime\", lag(col(\"dropoff_datetime\")).over(w))\n",
    "# df_taxi.show(2)\n",
    "\n",
    "\n",
    "# Compute idle time if gap ≤ 4 hours, else 0\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"idle_sec\",\n",
    "    when(\n",
    "        (col(\"prev_dropoff_datetime\").isNotNull()) &\n",
    "        (\n",
    "            (col(\"pickup_datetime\").cast(\"long\") - col(\"prev_dropoff_datetime\").cast(\"long\"))\n",
    "            <= 14400\n",
    "        ),\n",
    "        col(\"pickup_datetime\").cast(\"long\") - col(\"prev_dropoff_datetime\").cast(\"long\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "# df_taxi.show(5)\n",
    "\n",
    "# Aggregate per medallion\n",
    "df_util = df_taxi.groupBy(\"medallion\").agg(\n",
    "    _sum(\"duration_sec\").alias(\"total_occupied_time\"),\n",
    "    _sum(\"idle_sec\").alias(\"total_idle_time\")\n",
    ")\n",
    "\n",
    "# Compute utilization\n",
    "df_util = df_util.withColumn(\n",
    "    \"utilization\",\n",
    "    expr(\"total_occupied_time / (total_occupied_time + total_idle_time)\")\n",
    ")\n",
    "\n",
    "print(\"=== Query 1: Utilization per Taxi/Driver ===\")\n",
    "df_util.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c93b5d-b1ba-4a8e-8514-8310182cf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Partition by medallion (the taxi ID) and order by dropoff time\n",
    "w = Window.partitionBy(\"medallion\").orderBy(\"dropoff_datetime\")\n",
    "\n",
    "# 2) For each trip, find the pickup time of the \"next\" trip for that same taxi\n",
    "df_taxi = df_taxi.withColumn(\"next_pickup_datetime\", lead(col(\"pickup_datetime\")).over(w))\n",
    "\n",
    "# 3) Compute gap in seconds = next pickup - current dropoff\n",
    "#    Because subtracting two Timestamp columns in Spark 3+ yields an Interval,\n",
    "#    we convert them to numeric (seconds) first (using either cast(\"long\") or unix_timestamp).\n",
    "df_taxi = df_taxi.withColumn(\n",
    "    \"gap_to_next_sec\",\n",
    "    unix_timestamp(\"next_pickup_datetime\") - unix_timestamp(\"dropoff_datetime\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6740d5da-abe8-445d-98bb-3025a9f407d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+------------+---------------------+--------+--------------------+---------------+\n",
      "|           medallion|    pickup_datetime|   dropoff_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|pickup_borough|dropoff_borough|duration_sec|prev_dropoff_datetime|idle_sec|next_pickup_datetime|gap_to_next_sec|\n",
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+------------+---------------------+--------+--------------------+---------------+\n",
      "|0038EF45118925A51...|2013-01-01 00:07:00|2013-01-01 00:15:00|      -73.961807|      40.759731|       -73.977745|       40.749638|     Manhattan|      Manhattan|         480|                 NULL|       0| 2013-01-01 00:17:00|            120|\n",
      "|0038EF45118925A51...|2013-01-01 00:17:00|2013-01-01 00:28:00|      -73.981544|      40.747299|       -73.990036|       40.766998|     Manhattan|      Manhattan|         660|  2013-01-01 00:15:00|     120| 2013-01-01 00:29:00|             60|\n",
      "|0038EF45118925A51...|2013-01-01 00:29:00|2013-01-01 00:46:00|      -73.987312|      40.770901|       -73.959686|        40.76701|     Manhattan|      Manhattan|        1020|  2013-01-01 00:28:00|      60| 2013-01-01 00:48:00|            120|\n",
      "+--------------------+-------------------+-------------------+----------------+---------------+-----------------+----------------+--------------+---------------+------------+---------------------+--------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (Optionally) filter out negative or unrealistically large gaps if needed:\n",
    "df_taxi_valid_gap = df_taxi.filter(\n",
    "    (col(\"gap_to_next_sec\").isNotNull()) &\n",
    "    (col(\"gap_to_next_sec\") >= 0) &\n",
    "    (col(\"gap_to_next_sec\") <= 14400)  # e.g. max 4 hours, if desired\n",
    ")\n",
    "df_taxi_valid_gap.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095afd18-d427-413c-8c2a-406af31c2e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------------+\n",
      "|dropoff_borough|avg_time_to_next_fare_sec|\n",
      "+---------------+-------------------------+\n",
      "|Queens         |2807.452292499565        |\n",
      "|Unknown        |1235.5707161031012       |\n",
      "|Brooklyn       |1650.6873404056078       |\n",
      "|Staten Island  |3016.0944854232525       |\n",
      "|Manhattan      |728.0687218132239        |\n",
      "|Bronx          |2335.052113529537        |\n",
      "+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Group by the dropoff borough and compute average gap\n",
    "df_borough_gap = df_taxi_valid_gap.groupBy(\"dropoff_borough\") \\\n",
    "    .agg(avg(\"gap_to_next_sec\").alias(\"avg_time_to_next_fare_sec\"))\n",
    "\n",
    "df_borough_gap.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733bbfa3-a4d9-491e-9ff8-c05cf6759d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query 3: Number of Trips Starting & Ending in the Same Borough ===\n",
      "Count = 13108532\n"
     ]
    }
   ],
   "source": [
    "df_same_borough = df_taxi.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "same_borough_count = df_same_borough.count()\n",
    "\n",
    "print(\"=== Query 3: Number of Trips Starting & Ending in the Same Borough ===\")\n",
    "print(f\"Count = {same_borough_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "779e7a0a-520d-4acf-85b4-3c715746b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query 4: Number of Trips Starting in One Borough & Ending in Another ===\n",
      "Count = 1667722\n"
     ]
    }
   ],
   "source": [
    "df_diff_borough = df_taxi.filter(col(\"pickup_borough\") != col(\"dropoff_borough\"))\n",
    "diff_borough_count = df_diff_borough.count()\n",
    "\n",
    "print(\"=== Query 4: Number of Trips Starting in One Borough & Ending in Another ===\")\n",
    "print(f\"Count = {diff_borough_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd6bf8ca-54f5-4769-9c2b-10c8c8557d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19978010-5df9-438c-b42b-a0601be43871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861ecda-5335-4ba2-a496-e0403db8e857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
