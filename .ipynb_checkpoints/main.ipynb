{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPNGXbbKPMRJ"
   },
   "source": [
    "# Environment prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sygNLpVOERC",
    "outputId": "07f36f2d-a2b2-46c8-f81d-e1423902bc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfiETFz0Or6J",
    "outputId": "d281f28f-d05d-45bb-9b0b-b1b5e3639601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.1.4-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Downloading kafka_python-2.1.4-py2.py3-none-any.whl (276 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mBpIcxlO3dQ",
    "outputId": "1e78e8d9-8435-4cd5-f799-254d97bc348a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, udf, count, rank, lit, when, struct, collect_list, median, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BDM_project_2(DEBS2015-TaxiChallenge)\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "85nGNasKPCYo"
   },
   "outputs": [],
   "source": [
    "# Define schema for taxi data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8Ub_lKBPJPS"
   },
   "source": [
    "# Data cleaning (Query 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n0Dco0UPHTc",
    "outputId": "571940cc-aadf-4b6e-ea00-b18baef8ec46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0 - Sample of Cleaned Data:\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|945F1F65FAA293DA1...|D7421B620BD448E6B...|2013-02-17 16:13:00|2013-02-17 16:23:00|              600|         1.31|      -73.974167|      40.753681|       -73.990501|       40.751266|         CRD|        7.5|      0.0|    0.5|       1.5|         0.0|\n",
      "|94E10E3A3763877CE...|826FF187797D81521...|2013-02-17 16:01:00|2013-02-17 16:23:00|             1320|         3.67|      -73.972206|      40.754059|       -73.986916|       40.750572|         CRD|       17.5|      0.0|    0.5|       3.5|         0.0|\n",
      "|9B6A7942D02E1977A...|3E2D2C56FFAFFCAA7...|2013-02-17 15:59:00|2013-02-17 16:23:00|             1440|         4.17|      -73.966209|      40.770863|       -74.005684|        40.72559|         CSH|       18.0|      0.0|    0.5|       0.0|         0.0|\n",
      "|A1627FA9AB9437855...|4BB61985C755BF1BC...|2013-02-17 16:09:00|2013-02-17 16:23:00|              840|          2.6|      -73.970741|      40.788445|       -73.986275|       40.756046|         CRD|       11.5|      0.0|    0.5|       2.3|         0.0|\n",
      "|46BC499CF11522E49...|11BAF36F141401322...|2013-02-17 16:21:03|2013-02-17 16:23:11|              127|          0.4|      -73.979561|      40.752693|       -73.972618|       40.762199|         CSH|        3.5|      0.0|    0.5|       0.0|         0.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./sorted_data_1gb\"\n",
    "df = spark.read.csv(data_path, schema = schema, header = True)\n",
    "\n",
    "cleaned_df = df.filter(\n",
    "    (col(\"pickup_longitude\").isNotNull()) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) &\n",
    "    (col(\"pickup_longitude\") != 0) &\n",
    "    (col(\"pickup_latitude\") != 0) &\n",
    "    (col(\"dropoff_longitude\") != 0) &\n",
    "    (col(\"dropoff_latitude\") != 0) &\n",
    "    (col(\"medallion\").isNotNull()) &\n",
    "    (col(\"hack_license\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") > 0)\n",
    ")\n",
    "\n",
    "print(\"Query 0 - Sample of Cleaned Data:\")\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XEFlFuPYHaK",
    "outputId": "133787ed-5a8d-4783-df9c-3ca8823d55c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Cell Sample:\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|pickup_grid_500|dropoff_grid_500|pickup_grid_250|dropoff_grid_250|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "|       -160.157|        -160.154|       -320.313|        -321.308|\n",
      "|       -160.157|        -160.155|       -320.314|        -321.309|\n",
      "|       -156.158|        -166.152|       -312.316|        -332.303|\n",
      "|       -152.157|        -159.155|       -304.314|        -319.309|\n",
      "|       -160.156|        -158.157|       -320.311|        -316.314|\n",
      "+---------------+----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_grid_cell(lat, lon, cell_size_m):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    origin_lat, origin_lon = 41.474937, -74.913585\n",
    "    lat_per_cell = cell_size_m / 111000.0\n",
    "    lon_per_cell = cell_size_m / (111000.0 * math.cos(math.radians(origin_lat)))\n",
    "    lat_offset = math.floor((lat - origin_lat) / lat_per_cell) + 1\n",
    "    lon_offset = math.floor((lon - origin_lon) / lon_per_cell) + 1\n",
    "    return f\"{lat_offset}.{lon_offset}\"\n",
    "\n",
    "grid_cell_500_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 500), StringType())\n",
    "grid_cell_250_udf = udf(lambda lat, lon: get_grid_cell(lat, lon, 250), StringType())\n",
    "\n",
    "# Grid transformation\n",
    "df_grid = cleaned_df \\\n",
    "    .withColumn(\"pickup_grid_500\", grid_cell_500_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_500\", grid_cell_500_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_500\").isNotNull() & col(\"dropoff_grid_500\").isNotNull() &\n",
    "            col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "print(\"Grid Cell Sample:\")\n",
    "df_grid.select(\"pickup_grid_500\", \"dropoff_grid_500\", \"pickup_grid_250\", \"dropoff_grid_250\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMWpqamXRFuk"
   },
   "source": [
    "# Query 1 - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|start_cell|end_cell|ride_count|\n",
      "+----------+--------+----------+\n",
      "|  -156.160|-159.154|         2|\n",
      "|  -154.155|-153.157|         2|\n",
      "|  -153.160|-157.158|         2|\n",
      "|  -149.158|-156.155|         1|\n",
      "|  -157.157|-153.156|         1|\n",
      "|  -170.151|-157.160|         1|\n",
      "|  -149.158|-148.159|         1|\n",
      "|  -156.175|-165.153|         1|\n",
      "|  -154.161|-162.156|         1|\n",
      "|  -157.159|-157.157|         1|\n",
      "+----------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a fixed reference timestamp\n",
    "reference_time = df_grid.agg(F.max(\"dropoff_datetime\")).collect()[0][0]\n",
    "\n",
    "# Filter the data for trips that occurred in the last 30 minutes relative to the reference time\n",
    "time_limit = F.lit(reference_time) - F.expr(\"INTERVAL 30 MINUTES\")\n",
    "\n",
    "# Filter the DataFrame to include only trips completed in the last 30 minutes from the reference time\n",
    "df_recent = df_grid.filter(F.col(\"dropoff_datetime\") >= time_limit)\n",
    "\n",
    "# Perform the groupBy aggregation to find frequent routes within the last 30 minutes\n",
    "route_counts = df_recent \\\n",
    "    .groupBy(\"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(F.count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank routes by their frequency\n",
    "window_spec = Window.orderBy(F.col(\"ride_count\").desc())\n",
    "\n",
    "# Apply the window function to rank the routes\n",
    "top_10_routes_static = route_counts \\\n",
    "    .withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") <= 10)\n",
    "\n",
    "# Select only the relevant columns for output: start cell, end cell, and number of rides\n",
    "top_10_routes_static = top_10_routes_static.select(\n",
    "    F.col(\"pickup_grid_500\").alias(\"start_cell\"),\n",
    "    F.col(\"dropoff_grid_500\").alias(\"end_cell\"),\n",
    "    \"ride_count\"\n",
    ")\n",
    "\n",
    "# Show the top 10 frequent routes\n",
    "top_10_routes_static.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1 - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+--------+---------+\n",
      "|    pickup_datetime|   dropoff_datetime|start_cell|end_cell|    delay|\n",
      "+-------------------+-------------------+----------+--------+---------+\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -152.157|-150.157|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -157.158|-164.153|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -144.161|-145.161|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -151.157|-134.165|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -151.160|-159.157|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -155.161|-156.161|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -156.154|-165.154|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -156.158|-150.159|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -156.158|-167.159|386371750|\n",
      "|2013-01-01 02:00:00|2013-01-01 02:30:00|  -157.155|-159.158|386371750|\n",
      "+-------------------+-------------------+----------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grid = df_grid.withColumn(\"event_time\", col(\"dropoff_datetime\"))\n",
    "\n",
    "# Apply watermarking to track events dynamically\n",
    "windowed_routes = df_grid \\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"30 minutes\"), \"pickup_grid_500\", \"dropoff_grid_500\") \\\n",
    "    .agg(count(\"*\").alias(\"ride_count\"))\n",
    "\n",
    "# Rank routes dynamically within each 30-minute window\n",
    "window_spec = Window.partitionBy(\"window\").orderBy(col(\"ride_count\").desc())\n",
    "\n",
    "top_routes = windowed_routes \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Processing delay\n",
    "top_routes = top_routes.withColumn(\"delay\", (current_timestamp() - col(\"window.start\")).cast(\"long\"))\n",
    "\n",
    "final_result = top_routes.select(\n",
    "    col(\"window.start\").alias(\"pickup_datetime\"),\n",
    "    col(\"window.end\").alias(\"dropoff_datetime\"),\n",
    "    col(\"pickup_grid_500\").alias(\"start_cell\"),\n",
    "    col(\"dropoff_grid_500\").alias(\"end_cell\"),\n",
    "    col(\"delay\")\n",
    ")\n",
    "\n",
    "final_result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJPwCt4AWZXW"
   },
   "source": [
    "# Query 2 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k81v4PM6f4cs",
    "outputId": "f8ac875c-4ed3-46e8-f414-a32d33cce2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2 Part 1 - Top 10 Profitable Areas:\n",
      "+--------+-----------+-------------+-------------+----+\n",
      "| cell_id|empty_taxis|median_profit|profitability|rank|\n",
      "+--------+-----------+-------------+-------------+----+\n",
      "|-128.590|          0|       504.77|       504.77|   1|\n",
      "|-386.228|          0|        300.0|        300.0|   2|\n",
      "|-256.372|          0|        296.4|        296.4|   3|\n",
      "|-173.480|          1|        288.0|        288.0|   4|\n",
      "|-365.113|          1|        288.0|        288.0|   4|\n",
      "| -387.75|          0|        273.0|        273.0|   6|\n",
      "|-157.512|          0|        270.0|        270.0|   7|\n",
      "|-332.283|          1|        260.0|        260.0|   8|\n",
      "|-307.454|          0|        242.0|        242.0|   9|\n",
      "|-186.456|          0|        241.2|        241.2|  10|\n",
      "+--------+-----------+-------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, median, count, when, rank\n",
    "\n",
    "# Add profit column and 250m grid cells\n",
    "profit_df = cleaned_df \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "    .filter(col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "median_profit = profit_df \\\n",
    "    .groupBy(\"pickup_grid_250\") \\\n",
    "    .agg(median(\"profit\").alias(\"median_profit\"))\n",
    "\n",
    "empty_taxis = profit_df \\\n",
    "    .groupBy(\"medallion\", \"dropoff_grid_250\", \"dropoff_datetime\") \\\n",
    "    .agg(count(\"*\").alias(\"trip_count\")) \\\n",
    "    .groupBy(\"dropoff_grid_250\") \\\n",
    "    .agg(count(\"medallion\").alias(\"empty_taxis\"))\n",
    "\n",
    "profitability = median_profit \\\n",
    "    .join(empty_taxis, median_profit.pickup_grid_250 == empty_taxis.dropoff_grid_250, \"left_outer\") \\\n",
    "    .na.fill({\"empty_taxis\": 0}) \\\n",
    "    .withColumn(\"profitability\",\n",
    "                when(col(\"empty_taxis\") > 0, col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "                .otherwise(col(\"median_profit\"))) \\\n",
    "    .select(col(\"pickup_grid_250\").alias(\"cell_id\"), \"empty_taxis\", \"median_profit\", \"profitability\")\n",
    "\n",
    "window_spec_profit = Window.orderBy(col(\"profitability\").desc())\n",
    "top_10_profit_static = profitability \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec_profit)) \\\n",
    "    .filter(col(\"rank\") <= 10) \\\n",
    "    .select(\"cell_id\", \"empty_taxis\", \"median_profit\", \"profitability\", \"rank\")\n",
    "\n",
    "# Show results\n",
    "print(\"Query 2 Part 1 - Top 10 Profitable Areas:\")\n",
    "top_10_profit_static.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_top_10_profit = []\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, lit, when, rank, expr, current_timestamp, unix_timestamp, lead\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime  # ✅ Needed for delay calculation\n",
    "\n",
    "\n",
    "def process_profitability_batch(batch_df, batch_id):\n",
    "    global previous_top_10_profit\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Get latest timestamp\n",
    "    latest_time = batch_df.agg(F.max(\"dropoff_datetime\")).collect()[0][0]\n",
    "\n",
    "    # Calculate profit\n",
    "    profit_df = batch_df \\\n",
    "        .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "        .withColumn(\"pickup_grid_250\", grid_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "        .withColumn(\"dropoff_grid_250\", grid_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "        .filter(col(\"pickup_grid_250\").isNotNull() & col(\"dropoff_grid_250\").isNotNull())\n",
    "\n",
    "    # Profit filter (last 15 mins)\n",
    "    profit_df_filtered = profit_df.filter(col(\"pickup_datetime\") >= (lit(latest_time) - expr(\"INTERVAL 15 MINUTES\")))\n",
    "\n",
    "    # Median profit\n",
    "    median_profit = profit_df_filtered.groupBy(\"pickup_grid_250\") \\\n",
    "        .agg(F.expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"))\n",
    "\n",
    "    # Empty taxis = dropoffs in last 30 mins with no next pickup\n",
    "    empty_df = profit_df \\\n",
    "        .filter(col(\"dropoff_datetime\") >= (lit(latest_time) - expr(\"INTERVAL 30 MINUTES\"))) \\\n",
    "        .withColumn(\"next_pickup\", lead(\"pickup_datetime\").over(Window.partitionBy(\"medallion\").orderBy(\"dropoff_datetime\"))) \\\n",
    "        .filter(col(\"next_pickup\").isNull() | (col(\"next_pickup\") > col(\"dropoff_datetime\"))) \\\n",
    "        .groupBy(\"dropoff_grid_250\") \\\n",
    "        .agg(F.countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "\n",
    "    # Join profit + empty taxis → profitability\n",
    "    profitability = median_profit \\\n",
    "        .join(empty_df, median_profit.pickup_grid_250 == empty_df.dropoff_grid_250, \"left_outer\") \\\n",
    "        .na.fill({\"empty_taxis\": 0}) \\\n",
    "        .withColumn(\"profitability\", \n",
    "            when(col(\"empty_taxis\") > 0, col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "            .otherwise(col(\"median_profit\"))) \\\n",
    "        .select(col(\"pickup_grid_250\").alias(\"cell_id\"), \"empty_taxis\", \"median_profit\", \"profitability\")\n",
    "\n",
    "    # Top 10 by profitability\n",
    "    ranked = profitability.withColumn(\"rank\", rank().over(Window.orderBy(col(\"profitability\").desc()))) \\\n",
    "        .filter(col(\"rank\") <= 10).orderBy(\"rank\").collect()\n",
    "\n",
    "    # Check if top 10 changed\n",
    "    current_top_10 = [r[\"cell_id\"] for r in ranked]\n",
    "    if current_top_10 != previous_top_10_profit:\n",
    "        previous_top_10_profit = current_top_10\n",
    "\n",
    "        # Format output\n",
    "        output = [str(latest_time)] * 2  # pickup_datetime, dropoff_datetime\n",
    "        for i in range(10):\n",
    "            if i < len(ranked):\n",
    "                r = ranked[i]\n",
    "                output.extend([\n",
    "                    r[\"cell_id\"],\n",
    "                    str(r[\"empty_taxis\"]),\n",
    "                    str(round(r[\"median_profit\"], 2)),\n",
    "                    str(round(r[\"profitability\"], 4))\n",
    "                ])\n",
    "            else:\n",
    "                output.extend([\"NULL\", \"NULL\", \"NULL\", \"NULL\"])\n",
    "\n",
    "        # Compute delay (simplified for demo)\n",
    "        delay = int((datetime.now() - latest_time).total_seconds())\n",
    "        output.append(str(delay))\n",
    "\n",
    "        print(\"Updated Top 10 Profitable Areas:\")\n",
    "        print(\",\".join(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark.readStream \\\n",
    "    .option(\"header\", False) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"sorted_data_1gb/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_stream = stream_df.writeStream \\\n",
    "    .foreachBatch(process_profitability_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query2_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
